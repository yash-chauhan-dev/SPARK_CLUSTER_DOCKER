# Set-up Apache Pyspark Cluster Locally using Docker

This repository provides a **Docker-based Apache Spark cluster** setup with:
- **Spark Master**
- **Multiple Spark Workers**
- **Spark History Server**
- **HDFS**
- **Airflow**
- **PostgreSQL**

## üöÄ Setup & Running the Cluster

### 1Ô∏è‚É£ **Build the Spark Image**
```bash
docker-compose build
```

### 2Ô∏è‚É£ **Start the Spark Cluster**
```bash
docker-compose up -d
```
This starts:
- **Spark Master** (`spark://spark-master:7077`)
- **Spark Worker(s)**
- **Spark History Server**

### 3Ô∏è‚É£ **Check Running Containers**
```bash
docker ps
```

### 4Ô∏è‚É£ **Scale Up/Down Workers**
To start **3 worker nodes**:
```bash
docker-compose up -d --scale spark-worker=3
```
To scale dynamically:
```bash
docker-compose up -d --scale spark-worker=5  # Increase to 5 workers
docker-compose up -d --scale spark-worker=2  # Reduce to 2 workers
```

### 5Ô∏è‚É£ **Access Spark Web UI**
- **Spark Master UI** ‚Üí [http://localhost:9090](http://localhost:9090)
- **Spark History Server UI** ‚Üí [http://localhost:18080](http://localhost:18080)

### 6Ô∏è‚É£ **Submit an ETL Job**
To run an **ETL pipeline** (e.g., `main.py`):
```bash
docker exec -it da-spark-master spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  /path/to/main.py
```
To pass arguments:
```bash
docker exec -it da-spark-master spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  /path/to/main.py arg1 arg2
```

### 7Ô∏è‚É£ **Stop & Remove the Cluster**
To **stop** the cluster (without removing containers):
```bash
docker-compose stop
```
To **stop & remove** all containers:
```bash
docker-compose down
```
To **remove everything** (including volumes):
```bash
docker-compose down -v
```

## Working with PostgreSQL

### 1Ô∏è‚É£ **Access PostgreSQL database inside the container**
```bash
docker exec -it postgres-db psql -U user -d airbnb
```
### 2Ô∏è‚É£ **Run a SQL command inside PostgreSQL**
```bash
docker exec -it postgres-db psql -U user -d airbnb -c "SELECT * FROM table_name;"
```

## Managing Hadoop (HDFS)

### 1Ô∏è‚É£ **Format the NameNode (Only for first-time setup)**
```bash
docker exec -it namenode hdfs namenode -format
```

### 2Ô∏è‚É£ **List HDFS files**
```bash
docker exec -it namenode hdfs dfs -ls /
```

### 3Ô∏è **Upload a file to HDFS**
```bash
docker exec -it namenode hdfs dfs -put /local/path/file.txt /hdfs/path/
```

## Managing Airflow

### 1Ô∏è‚É£ **Access Airflow web UI**
- Airflow UI: http://localhost:8081

### 2Ô∏è‚É£ **Manually trigger an Airflow DAG**
```bash
docker exec -it airflow airflow dags trigger <dag_id>
```

### 3Ô∏è‚É£ **Check Airflow DAGs**
```bash
docker exec -it airflow airflow dags list
```

### 4Ô∏è‚É£ **Restart Airflow**
```bash
docker-compose restart airflow
```

## üìù Note
- Ensure your ETL scripts (`spark_session.py`, `extract.py`, `transform.py`, `load.py`, `main.py`) are inside the container or mounted.
- Use `spark-submit` to run your jobs.
- Dynamically **scale workers** based on workload.

üöÄ **Now you're ready to run your Apache Spark cluster with Docker!**

## Youtube Video Link

[![Youtube Link](https://img.youtube.com/vi/Mrp1VAa_HL0/0.jpg)](https://www.youtube.com/watch?v=Mrp1VAa_HL0)
